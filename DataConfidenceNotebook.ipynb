{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a product called Informatica Analyst (https://kb.informatica.com/proddocs/Product%20Documentation/4/IN_90_Analyst_UserGuide_en.pdf) which does perform the functions below and a great deal more.\n",
    "Informatica Analyst is a licenced application, thus usage requires a cost.\n",
    "\n",
    "Microsoft Azure also have an offering in this space : https://gallery.cortanaintelligence.com/Experiment/24c4e869c5c448958ce923c2e2bfbb27\n",
    "\n",
    "The functionality below is free and open, so you can modify it to do whatever you wish. I would encourage you to contribute back to the community though, by adding any new functionality back in to the trunk / parent version.\n",
    "\n",
    "The driver used for connecting to postgres was \n",
    "conda install -c anaconda sqlalchemy=1.1.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "import dataQualityLibrary as dql\n",
    "dql.sayHello()\n",
    "\n",
    "#plotly.tools.set_credentials_file(username='odonovan.johnd@gmail.com', api_key='***************')\n",
    "\n",
    "'''\n",
    "Level\tNumeric value\n",
    "CRITICAL\t50\n",
    "ERROR\t40\n",
    "WARNING\t30\n",
    "INFO\t20\n",
    "DEBUG\t10\n",
    "NOTSET\t0\n",
    "'''\n",
    "\n",
    "true=1\n",
    "false=0\n",
    "\n",
    "offline_mode = true\n",
    "\n",
    "\n",
    "dql.setLoggingLevel (10)\n",
    "\n",
    "# The following line allows us to run Plotly locally, for the puposed of this blog it's easier to \n",
    "# operate in offline mode.\n",
    "if offline_mode:\n",
    "    offline.init_notebook_mode()\n",
    "\n",
    "logging.info (\"Running version \" + plotly.__version__)\n",
    "\n",
    "class workWithThis (object):\n",
    "    def __init__(self):\n",
    "        print ('')\n",
    "\n",
    "\n",
    "smallTest = workWithThis()\n",
    "smallTest.workWithDb = false\n",
    "smallTest.dataSource = 'smallTest'\n",
    "smallTest.duplicateColumns = ['col2']\n",
    "smallTest.anomolousNumericColumns = ['col2']\n",
    "smallTest.anomolousTextColumns = ['col7']\n",
    "smallTest.columnsUniqueGraph = []\n",
    "\n",
    "\n",
    "gpobi_xio_json_1000 = workWithThis()\n",
    "gpobi_xio_json_1000.workWithDb = false\n",
    "gpobi_xio_json_1000.dataSource = 'gpobi_xio_json_1000'\n",
    "gpobi_xio_json_1000.duplicateColumns = ['tla']\n",
    "gpobi_xio_json_1000.anomolousNumericColumns = ['iops']\n",
    "gpobi_xio_json_1000.anomolousTextColumns = ['ssd_uid']\n",
    "gpobi_xio_json_1000.columnsUniqueGraph = ['swap_led', 'part_number', 'encryption_status', 'product_model']\n",
    "\n",
    "\n",
    "gpobi_xio_json_db = workWithThis()\n",
    "gpobi_xio_json_db.workWithDb = true\n",
    "gpobi_xio_json_db.dataSource = 'gpobi_xio_json'\n",
    "gpobi_xio_json_db.duplicateColumns = ['tla']\n",
    "gpobi_xio_json_db.anomolousNumericColumns = ['iops']\n",
    "gpobi_xio_json_db.anomolousTextColumns = ['ssd_uid']\n",
    "gpobi_xio_json_db.columnsUniqueGraph = ['swap_led', 'part_number', 'encryption_status', 'product_model']\n",
    "\n",
    "unity_drive_data = workWithThis()\n",
    "unity_drive_data.workWithDb = false\n",
    "unity_drive_data.dataSource = 'unity_drive_data_1000'\n",
    "unity_drive_data.duplicateColumns = ['tla_serial_num']\n",
    "unity_drive_data.anomolousNumericColumns = ['drive_part_num']\n",
    "unity_drive_data.anomolousTextColumns = ['drive_prod_id']\n",
    "unity_drive_data.columnsUniqueGraph = ['product_model','product_revision', 'disk_status', 'config_type']\n",
    "\n",
    "vnx2_drive_data = workWithThis()\n",
    "vnx2_drive_data.workWithDb = false\n",
    "vnx2_drive_data.dataSource = 'vnx2_drive_data_1000'\n",
    "#vnx2_drive_data.dataSource = 'vnx2_drive_data'\n",
    "vnx2_drive_data.duplicateColumns = ['tla_serial_number']\n",
    "vnx2_drive_data.anomolousNumericColumns = ['drive_part_num', 'bms_total_scans', 'write_tot_bytes_proc']\n",
    "vnx2_drive_data.anomolousTextColumns = ['product_revision']\n",
    "vnx2_drive_data.columnsUniqueGraph = ['product_model', 'drive_prod_id', 'config_type']\n",
    "\n",
    "# So we have setup our objects above, now we can just pick which to use\n",
    "#\n",
    "workWithThis = gpobi_xio_json_1000; # Verified\n",
    "#workWithThis= gpobi_xio_json_db;\n",
    "#workWithThis = smallTest; # Verified\n",
    "#workWithThis = vnx2_drive_data; # Verified\n",
    "#workWithThis = unity_drive_data; # Verified\n",
    "dql.printSeperator();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not workWithThis.workWithDb:\n",
    "    logging.info ('working with file')\n",
    "    \n",
    "    # Let's load two of the files and graph them\n",
    "    df = pd.read_csv (\"/Users/odonoj7/Syncplicity/IMI/FridayAfternoonTest/\" + workWithThis.dataSource  + \".csv\")\n",
    " \n",
    "else:\n",
    "    logging.info ('working with database')\n",
    "    import sqlalchemy as sq\n",
    "    import myconfig as props\n",
    "\n",
    "    logging.debug ('before connection')\n",
    "  \n",
    "    #engine = sq.create_engine(\"postgresql+psycopg2://odonoj7:p4ssw0rd@localhost:5432/odonoj7\")\n",
    "    engine = sq.create_engine(\"postgresql+psycopg2://\"+props.dbusername+\":\"+props.dbpassword+\"@\"+props.dbhost+\":\"+props.dbport+\"/\"+props.dbname)\n",
    "    logging.debug ('after connection')\n",
    "\n",
    "    # Returning large volumes of data from the database to here is not a good use of the technologies available\n",
    "    # This will lead to a large amount of data being transferred to the memory of the process running Jupyter\n",
    "    #\n",
    "    try:\n",
    "        df = pd.read_sql_query ('select * from emcas_gpo_mfg.' + workWithThis.dataSource + ' limit 100000', engine)\n",
    "        logging.info ('We are working with ' + str(df.shape[1]) + ' columns and ' + str(df.shape[0]) + ' rows')\n",
    "    except Exception as inst:\n",
    "        logging.error(type(inst))    # the exception instance\n",
    "        logging.error(inst.args)     # arguments stored in .args\n",
    "        logging.error(inst)\n",
    "        logging.info ('unable to connect to database, no dataframe was loaded.')\n",
    "        \n",
    "\n",
    "    # We now have a database connection to access data from this database we can use\n",
    "\n",
    "    \n",
    "dql.printSeperator();    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Finding Text outliers</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.info ('finding text outliers in ' + str(workWithThis.anomolousTextColumns))\n",
    "\n",
    "if workWithThis.anomolousTextColumns.count != 0:\n",
    "    for col in workWithThis.anomolousTextColumns:\n",
    "        if col in df.columns.get_values():\n",
    "            dql.findTextLengthOutliers (df, col, true)\n",
    "        else:\n",
    "            logging.error ('Column not found ' + col)\n",
    "            \n",
    "dql.printSeperator();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Unique Column Value Graphs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This could all simply come from the properties file\n",
    "# Having it here enhances the interative experience\n",
    "for col in workWithThis.columnsUniqueGraph:\n",
    "    if col in df.columns.get_values():\n",
    "        dql.pieChartUniqueValues (df, col, offline_mode)\n",
    "    else:\n",
    "        logging.error ('Column not found ' + col)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Anonomolous Numeric Columns</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if workWithThis.anomolousNumericColumns.count != 0:\n",
    "    for col in workWithThis.anomolousNumericColumns:\n",
    "        if col in df.columns:\n",
    "            dql.findNumericAnomolies (df, col, 0)\n",
    "        else:\n",
    "            logging.error (col + 'column not available in this dataset')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Duplicate columns for user supplied columns</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('Checking for duplicates in these columns ' + str(workWithThis.duplicateColumns))\n",
    "\n",
    "print ('The following columns have duplicates')\n",
    "\n",
    "# Need some way to check that all the duplicated columns exist\n",
    "#\n",
    "allColsAvailable = true\n",
    "for col in workWithThis.duplicateColumns:\n",
    "    if not col in df.columns.get_values():\n",
    "        print ('Column not found ' + col)\n",
    "        allColsAvailable = false;\n",
    "if allColsAvailable:\n",
    "    print (df[df.duplicated(workWithThis.duplicateColumns)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Empty Columns</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the number of empty columns\n",
    "#\n",
    "withData=df.dropna(axis=1,how='all')\n",
    "print ('Number of columns with data ' + str(len(withData.columns)))\n",
    "\n",
    "columnsWithNoData = df.columns.difference (withData.columns);\n",
    "\n",
    "if len(columnsWithNoData) == 0:\n",
    "    logging.info ('All columns have some values')\n",
    "else:\n",
    "    dql.drawPie (['Columns With Data', 'Columns without Data : ' + str(columnsWithNoData.get_values())], [len(withData.columns),len(columnsWithNoData)], 'Empty Columns of (' + str(df.columns.size) + ') for ' + workWithThis.dataSource, 'true')\n",
    "    #dql.drawPie (['Columns With Data', 'Columns without Data : '], [len(withData.columns),len(columnsWithNoData)], 'Empty Columns of (' + str(df.columns.size) + ') for ' + workWithThis.dataSource, 'true')\n",
    "    logging.info (\"Columns with no data : \")\n",
    "    logging.info (columnsWithNoData.get_values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Number of missing values per column (Columns with no data have been removed)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FIND THE NUMBER OF MISSING FIELDS BY COLUMN\n",
    "# To show all columns use this line\n",
    "#baseSum = df.isnull().sum(axis=0);\n",
    "# To show only columns which have some data use this line\n",
    "#    - This leads to a much cleaner graph\n",
    "baseSum = withData.isnull().sum(axis=0);\n",
    "baseSum.name = \"numberOfMissingValues\"\n",
    "\n",
    "df1 = baseSum.to_frame()\n",
    "\n",
    "# This line will remove any columns where all values are 0\n",
    "#df1 = df1.loc[:, (df1 != 0).any(axis=0)]\n",
    "\n",
    "# In this case we want to remove all rows where the value is 0 for a cleaner graph showing\n",
    "# only rows that have missing values\n",
    "df1 = df1[(df1.T != 0).any()]\n",
    "\n",
    "if df1.empty:\n",
    "    print (\"There are no empty columns for this dataset (aside from empty columns))\" + workWithThis.dataSource)\n",
    "else:\n",
    "    # If we wanted to apply a transformation to a column this is how we could do it\n",
    "    #df1['numberOfMissingValues'] = df1['numberOfMissingValues'].apply(lambda x: (x/1000)*100)\n",
    "\n",
    "    data  = go.Data([\n",
    "                go.Bar(\n",
    "                  y = df1.numberOfMissingValues,\n",
    "                  x = df1.index\n",
    "            )])\n",
    "\n",
    "    layout = go.Layout(\n",
    "            height = '1000',\n",
    "            margin=go.Margin(b=100),\n",
    "            title = \"Number of incomplete fields in Columns for dataset (aside from empty columns) - \" + workWithThis.dataSource\n",
    "     )\n",
    "\n",
    "    fig  = go.Figure(data=data, layout=layout)\n",
    "    offline.iplot(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if df1.empty:\n",
    "    print (\"Where the columns is not empty, there are no empty rows for this dataset \" + workWithThis.dataSource)\n",
    "else:\n",
    "    mostCommonValue = df1['numberOfMissingValues'].value_counts().idxmax()\n",
    "    numberOfColsContainingMostCommonValue = df1.loc[df1['numberOfMissingValues'] == mostCommonValue]\n",
    "\n",
    "    print ('Where the column is not empty, the most common number of empty fields in a column is ' + str(mostCommonValue))\n",
    "    print ('The number of columns which match this is ' + str(len(numberOfColsContainingMostCommonValue)))\n",
    "\n",
    "    print (df1.loc[df1['numberOfMissingValues'] == mostCommonValue])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Number of fully populated rows</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Identify all empty rows\n",
    "rowsWithData=df.dropna(axis=0,how='all')\n",
    "rowsWithoutData = len(df) - len (rowsWithData)\n",
    "print ('the % of empty rows is ' + str(( rowsWithoutData / len (df)) * 100)  + \"% ( \"  + str(rowsWithoutData) + \" row(s) )\" )\n",
    "\n",
    "if not rowsWithoutData == 0:\n",
    "    dql.drawPie (['Rows with data', 'Rows without data'], [len (rowsWithData), rowsWithoutData], 'Empty Rows', 'true')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Number of partially populated rows<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now lets find the number of rows where at least one value is missing\n",
    "#partialRows=df.dropna(axis=0,how='any')\n",
    "partialRows = df[df.isnull().any(axis=1)]\n",
    "\n",
    "#partialRows = df.dropna(axis=0,how='all')\n",
    "\n",
    "\n",
    "logging.info ('the % partially complete rows ' + str(( partialRows.shape[0] / len (df)) * 100)  + \"% ( \"  + str(len (df) - partialRows.shape[0]) + \" row(s) )\" )\n",
    "if (partialRows.shape[0] != 0):\n",
    "    dql.drawPie (['Complete Data Row', 'Incomplete Data Row'], [ len (df) - partialRows.shape[0], len(df)], 'Complete Rows', 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Number of partially populated rows when problematic columns removed<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we remove the most common error columns, let's see if we have any complete rows then\n",
    "#\n",
    "\n",
    "# This may still have some bugs with certain datasets, be delighted if you could correct them.\n",
    "# This works when there are no empty columns\n",
    "# This doesn't work where there are empty columns\n",
    "#\n",
    "if df1.empty:\n",
    "    logging.info (\"There are no empty columns for this dataset, unable to remove most common error columns \" + workWithThis.dataSource)\n",
    "else:\n",
    "    # Remove the empty columns\n",
    "    dfWithoutRecognisedColumns = df.drop (columnsWithNoData, axis=1)\n",
    "    # Remove the columns which have the most frequency of missing values\n",
    "    dfWithoutRecognisedColumns = dfWithoutRecognisedColumns.drop(numberOfColsContainingMostCommonValue.index, axis=1)\n",
    "    logging.debug (dfWithoutRecognisedColumns.shape)\n",
    "    \n",
    "    rowsWithCompleteData=dfWithoutRecognisedColumns.dropna(axis=0,how='any')\n",
    "    logging.debug (rowsWithCompleteData.shape)\n",
    "\n",
    "    logging.info ('the % rows with complete data when known problematic columns are removed is ' + str(( rowsWithCompleteData.shape[0] / len (df)) * 100)  + \"% ( \"  + str(rowsWithCompleteData.shape[0]) + \" row(s) )\" )\n",
    "\n",
    "    if len(rowsWithCompleteData) != 0:\n",
    "        dql.drawPie (['Complete Data Row (problematic columns removed)', 'Incomplete Data Row (problematic columns removed)'], [len (rowsWithCompleteData), len(df) - len (rowsWithCompleteData)], 'Row Data Completeness when problematic rows are removed', 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Find the number of not-found across the dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalCount = 0\n",
    "\n",
    "#for col in df.columns:\n",
    "for col in df.select_dtypes([np.object]).columns[1:]:\n",
    "    totalCount += df[col].str.contains ('not-found').sum()\n",
    "\n",
    "if totalCount == 0:\n",
    "    logging.info (\"The string 'not-found' is not present in this dataset \" + workWithThis.dataSource)\n",
    "else:\n",
    "    dql.drawPie ([\"Instances of 'not-Found'\", \"Normal values\"], [totalCount, (df.shape[0] * df.shape[1])], \"Instances of 'not-found' in dataset\", true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "cdd72e8d-5690-4f2b-872b-932dd1fcdb18": {
     "id": "cdd72e8d-5690-4f2b-872b-932dd1fcdb18",
     "prev": null,
     "regions": {
      "01e0a221-c3c6-488f-afa2-41a517736a47": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "01e0a221-c3c6-488f-afa2-41a517736a47"
      },
      "4b25f619-1010-4f01-895f-e9f84eff1844": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "id": "4b25f619-1010-4f01-895f-e9f84eff1844"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
